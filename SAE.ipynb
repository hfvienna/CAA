{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Should the code not run, try upgrading transformers in your env:\\npip install --upgrade transformers\\n\\n2. The next cell is a system path fix. It may or may not be necessary to run in your environment.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Should the code not run, try upgrading transformers in your env:\n",
    "pip install --upgrade transformers\n",
    "\n",
    "2. The next cell is a system path fix. It may or may not be necessary to run in your environment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/root/CAA/venv/lib/python3.10/site-packages', '/root/CAA/venv/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMER / ENVIRONMENT FIX\n",
    "import sys\n",
    "sys.path.append('/root/CAA/venv/lib/python3.10/site-packages')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from behaviors import ALL_BEHAVIORS, BASE_DIR, COORDINATE, get_vector_path\n",
    "from gemma_2_wrapper import Gemma2Wrapper\n",
    "from generate_vectors import generate_save_vectors_for_behavior\n",
    "import gemma_vector_analysis\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "NEURONPEDIA_API_KEY = os.getenv(\"NEURONPEDIA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE loaded successfully\n",
      "Loaded CAA vector for behavior: coordinate-other-ais\n",
      "Loaded CAA vector for behavior: corrigible-neutral-HHH\n",
      "Loaded CAA vector for behavior: hallucination\n",
      "Loaded CAA vector for behavior: myopic-reward\n",
      "Loaded CAA vector for behavior: survival-instinct\n",
      "Loaded CAA vector for behavior: sycophancy\n",
      "Loaded CAA vector for behavior: refusal\n",
      "Processed vector for behavior: coordinate-other-ais\n",
      "Cosine similarity: 0.8658\n",
      "Original L2 norm: 44.3626\n",
      "Processed L2 norm: 255.2028\n",
      "Original mean: 0.0521\n",
      "Processed mean: 0.1877\n",
      "Original std dev: 0.9230\n",
      "Processed std dev: 5.3146\n",
      "\n",
      "Processed vector for behavior: corrigible-neutral-HHH\n",
      "Cosine similarity: 0.8685\n",
      "Original L2 norm: 44.3626\n",
      "Processed L2 norm: 268.3082\n",
      "Original mean: 0.0128\n",
      "Processed mean: 0.0354\n",
      "Original std dev: 0.9243\n",
      "Processed std dev: 5.5909\n",
      "\n",
      "Processed vector for behavior: hallucination\n",
      "Cosine similarity: 0.8456\n",
      "Original L2 norm: 44.3626\n",
      "Processed L2 norm: 241.0766\n",
      "Original mean: 0.0288\n",
      "Processed mean: 0.1753\n",
      "Original std dev: 0.9240\n",
      "Processed std dev: 5.0205\n",
      "\n",
      "Processed vector for behavior: myopic-reward\n",
      "Cosine similarity: 0.8897\n",
      "Original L2 norm: 44.3626\n",
      "Processed L2 norm: 193.2522\n",
      "Original mean: 0.0039\n",
      "Processed mean: -0.0211\n",
      "Original std dev: 0.9244\n",
      "Processed std dev: 4.0269\n",
      "\n",
      "Processed vector for behavior: survival-instinct\n",
      "Cosine similarity: 0.8538\n",
      "Original L2 norm: 44.3626\n",
      "Processed L2 norm: 212.8642\n",
      "Original mean: -0.0186\n",
      "Processed mean: -0.0488\n",
      "Original std dev: 0.9242\n",
      "Processed std dev: 4.4354\n",
      "\n",
      "Processed vector for behavior: sycophancy\n",
      "Cosine similarity: 0.8502\n",
      "Original L2 norm: 44.3626\n",
      "Processed L2 norm: 207.0221\n",
      "Original mean: 0.0089\n",
      "Processed mean: 0.0609\n",
      "Original std dev: 0.9244\n",
      "Processed std dev: 4.3135\n",
      "\n",
      "Processed vector for behavior: refusal\n",
      "Cosine similarity: 0.8543\n",
      "Original L2 norm: 44.3626\n",
      "Processed L2 norm: 232.5123\n",
      "Original mean: -0.0370\n",
      "Processed mean: -0.2305\n",
      "Original std dev: 0.9237\n",
      "Processed std dev: 4.8396\n",
      "\n",
      "Average cosine similarity: 0.8611\n",
      "Saved processed vector for behavior coordinate-other-ais at /root/CAA/sae_vector/processed_coordinate-other-ais.pt\n",
      "Saved processed vector for behavior corrigible-neutral-HHH at /root/CAA/sae_vector/processed_corrigible-neutral-HHH.pt\n",
      "Saved processed vector for behavior hallucination at /root/CAA/sae_vector/processed_hallucination.pt\n",
      "Saved processed vector for behavior myopic-reward at /root/CAA/sae_vector/processed_myopic-reward.pt\n",
      "Saved processed vector for behavior survival-instinct at /root/CAA/sae_vector/processed_survival-instinct.pt\n",
      "Saved processed vector for behavior sycophancy at /root/CAA/sae_vector/processed_sycophancy.pt\n",
      "Saved processed vector for behavior refusal at /root/CAA/sae_vector/processed_refusal.pt\n",
      "CAA vector processing complete.\n"
     ]
    }
   ],
   "source": [
    "# FINAL CODE I USE TO CONTINUE RESEARCH\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "layer = 14\n",
    "\n",
    "sae_model_name = \"google/gemma-scope-2b-pt-res\"\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=sae_model_name,\n",
    "    filename=f\"layer_{layer}/width_16k/average_l0_84/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v).to(device) for k, v in params.items()}\n",
    "\n",
    "class JumpReLUSAE(nn.Module):\n",
    "    def __init__(self, d_model, d_sae):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
    "        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
    "        self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def encode(self, input_acts):\n",
    "        pre_acts = input_acts @ self.W_enc + self.b_enc\n",
    "        mask = (pre_acts > self.threshold)\n",
    "        acts = mask * torch.nn.functional.relu(pre_acts)\n",
    "        return acts\n",
    "\n",
    "    def decode(self, acts):\n",
    "        return acts @ self.W_dec + self.b_dec\n",
    "\n",
    "    def forward(self, acts):\n",
    "        acts = self.encode(acts)\n",
    "        recon = self.decode(acts)\n",
    "        return recon\n",
    "        \n",
    "    def process_caa_vector(self, caa_vector, scale_factor=4.2813):\n",
    "        caa_vector = caa_vector.to(self.W_enc.device)\n",
    "        decoder_bias = self.b_dec\n",
    "\n",
    "        # Apply scaling to the CAA vector\n",
    "        caa_vector = caa_vector * scale_factor\n",
    "\n",
    "        # Compute SAE(caa_vector + decoder_bias) and SAE(-caa_vector + decoder_bias)\n",
    "        positive_encoded = self.encode(caa_vector + decoder_bias)\n",
    "        negative_encoded = self.encode(-caa_vector + decoder_bias)\n",
    "        \n",
    "        positive_decoded = self.decode(positive_encoded)\n",
    "        negative_decoded = self.decode(negative_encoded)\n",
    "        \n",
    "        # Calculate (positive - negative) / 2\n",
    "        result_vector = (positive_decoded - negative_decoded) / 2\n",
    "        \n",
    "        # Calculate cosine similarity between result_vector and caa_vector\n",
    "        cosine_sim = F.cosine_similarity(result_vector.view(1, -1), caa_vector.view(1, -1), dim=1).item()\n",
    "        \n",
    "        return result_vector, cosine_sim\n",
    "\n",
    "def process_caa_vectors(sae, caa_vectors, scale_factor=4.2813):\n",
    "    processed_vectors = {}\n",
    "    similarities = []\n",
    "    \n",
    "    for behavior, caa_vector in caa_vectors.items():\n",
    "        caa_vector = caa_vector.to(device)\n",
    "        \n",
    "        # Process the CAA vector with scaling\n",
    "        processed_vector, cosine_sim = sae.process_caa_vector(caa_vector, scale_factor=scale_factor)\n",
    "        \n",
    "        similarities.append(cosine_sim)\n",
    "        \n",
    "        processed_vectors[behavior] = processed_vector.cpu()\n",
    "        \n",
    "        print(f\"Processed vector for behavior: {behavior}\")\n",
    "        print(f\"Cosine similarity: {cosine_sim:.4f}\")\n",
    "        print(f\"Original L2 norm: {torch.norm(caa_vector).item():.4f}\")\n",
    "        print(f\"Processed L2 norm: {torch.norm(processed_vector).item():.4f}\")\n",
    "        print(f\"Original mean: {torch.mean(caa_vector).item():.4f}\")\n",
    "        print(f\"Processed mean: {torch.mean(processed_vector).item():.4f}\")\n",
    "        print(f\"Original std dev: {torch.std(caa_vector).item():.4f}\")\n",
    "        print(f\"Processed std dev: {torch.std(processed_vector).item():.4f}\")\n",
    "        print()\n",
    "    \n",
    "    avg_similarity = np.mean(similarities)\n",
    "    print(f\"Average cosine similarity: {avg_similarity:.4f}\")\n",
    "    \n",
    "    return processed_vectors\n",
    "\n",
    "# Load the SAE model\n",
    "sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae.load_state_dict(pt_params)\n",
    "sae = sae.to(device)\n",
    "print(\"SAE loaded successfully\")\n",
    "\n",
    "# Load CAA vectors\n",
    "caa_vectors = {}\n",
    "for behavior in ALL_BEHAVIORS:\n",
    "    layer = 14\n",
    "    model_name_path = 'gemma-2-2b'\n",
    "    vector_path = gemma_vector_analysis.fix_vector_path(get_vector_path(behavior, layer, model_name_path))\n",
    "    normalized_dir = os.path.join(BASE_DIR, 'normalized_vectors', behavior)\n",
    "    \n",
    "    original_path = os.path.join(normalized_dir, f\"vec_layer_{layer}_{model_name_path}.pt\")\n",
    "    original_path = gemma_vector_analysis.fix_vector_path(original_path)\n",
    "    caa_vector = torch.load(original_path)\n",
    "    caa_vectors[behavior] = caa_vector\n",
    "    print(f\"Loaded CAA vector for behavior: {behavior}\")\n",
    "\n",
    "# Process CAA vectors with scaling\n",
    "processed_vectors = process_caa_vectors(sae, caa_vectors, scale_factor=4.2813)\n",
    "\n",
    "# Save processed vectors\n",
    "sae_vector_dir = '/root/CAA/sae_vector'\n",
    "if not os.path.exists(sae_vector_dir):\n",
    "    os.makedirs(sae_vector_dir)\n",
    "\n",
    "for behavior, processed_vector in processed_vectors.items():\n",
    "    save_path = os.path.join(sae_vector_dir, f\"processed_{behavior}.pt\")\n",
    "    torch.save(processed_vector, save_path)\n",
    "    print(f\"Saved processed vector for behavior {behavior} at {save_path}\")\n",
    "\n",
    "print(\"CAA vector processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CAA vector norm: 44.3626\n",
      "Processed vector norm for behavior coordinate-other-ais: 255.2028\n",
      "Normalized vector norm for behavior coordinate-other-ais: 44.3626\n",
      "Processed vector norm for behavior corrigible-neutral-HHH: 268.3082\n",
      "Normalized vector norm for behavior corrigible-neutral-HHH: 44.3626\n",
      "Processed vector norm for behavior hallucination: 241.0766\n",
      "Normalized vector norm for behavior hallucination: 44.3626\n",
      "Processed vector norm for behavior myopic-reward: 193.2522\n",
      "Normalized vector norm for behavior myopic-reward: 44.3626\n",
      "Processed vector norm for behavior survival-instinct: 212.8642\n",
      "Normalized vector norm for behavior survival-instinct: 44.3626\n",
      "Processed vector norm for behavior sycophancy: 207.0221\n",
      "Normalized vector norm for behavior sycophancy: 44.3626\n",
      "Processed vector norm for behavior refusal: 232.5123\n",
      "Normalized vector norm for behavior refusal: 44.3626\n",
      "Normalization of processed vectors complete.\n"
     ]
    }
   ],
   "source": [
    "# Normalize all processed vectors to the norm of one of the original CAA vectors\n",
    "\n",
    "# Get the norm of the first original CAA vector (they all have the same norm)\n",
    "example_caa_vector = next(iter(caa_vectors.values()))\n",
    "original_norm = torch.norm(example_caa_vector).item()\n",
    "print(f\"Original CAA vector norm: {original_norm:.4f}\")\n",
    "\n",
    "# Create a directory for the normalized vectors\n",
    "normalized_vector_dir = '/root/CAA/sae_vector_normalized'\n",
    "if not os.path.exists(normalized_vector_dir):\n",
    "    os.makedirs(normalized_vector_dir)\n",
    "\n",
    "# Normalize and save the processed vectors\n",
    "for behavior, processed_vector in processed_vectors.items():\n",
    "    # Compute the norm of the processed vector\n",
    "    processed_norm = torch.norm(processed_vector).item()\n",
    "    \n",
    "    # Normalize the processed vector to the original norm\n",
    "    normalized_vector = processed_vector * (original_norm / processed_norm)\n",
    "    \n",
    "    # Save the normalized vector\n",
    "    save_path = os.path.join(normalized_vector_dir, f\"normalized_{behavior}.pt\")\n",
    "    torch.save(normalized_vector, save_path)\n",
    "    \n",
    "    # Print norms\n",
    "    print(f\"Processed vector norm for behavior {behavior}: {processed_norm:.4f}\")\n",
    "    print(f\"Normalized vector norm for behavior {behavior}: {torch.norm(normalized_vector).item():.4f}\")\n",
    "\n",
    "print(\"Normalization of processed vectors complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Behavior: coordinate-other-ais\n",
      "Total norm: 159.8280\n",
      "Top 10 contributing features:\n",
      "  1. Feature 10414: 16.71% (value: 65.3287)\n",
      "  2. Feature 10781: 2.48% (value: 25.1675)\n",
      "  3. Feature 715: 2.40% (value: 24.7662)\n",
      "  4. Feature 11139: 2.17% (value: 23.5419)\n",
      "  5. Feature 2721: 2.03% (value: 22.7471)\n",
      "  6. Feature 11113: 1.50% (value: 19.5599)\n",
      "  7. Feature 4121: 1.30% (value: 18.2298)\n",
      "  8. Feature 11500: 1.28% (value: 18.1127)\n",
      "  9. Feature 294: 1.21% (value: 17.5502)\n",
      "  10. Feature 11967: 1.13% (value: 17.0095)\n",
      "Number of non-zero features: 437\n",
      "\n",
      "Behavior: corrigible-neutral-HHH\n",
      "Total norm: 153.9369\n",
      "Top 10 contributing features:\n",
      "  1. Feature 10781: 12.11% (value: 53.5612)\n",
      "  2. Feature 1582: 5.70% (value: 36.7570)\n",
      "  3. Feature 10414: 3.64% (value: 29.3652)\n",
      "  4. Feature 11113: 3.61% (value: 29.2565)\n",
      "  5. Feature 3253: 2.81% (value: 25.8125)\n",
      "  6. Feature 715: 2.36% (value: 23.6331)\n",
      "  7. Feature 4121: 1.96% (value: 21.5418)\n",
      "  8. Feature 14080: 1.83% (value: 20.8156)\n",
      "  9. Feature 15113: 1.70% (value: 20.0573)\n",
      "  10. Feature 11139: 1.53% (value: 19.0168)\n",
      "Number of non-zero features: 337\n",
      "\n",
      "Behavior: hallucination\n",
      "Total norm: 156.2961\n",
      "Top 10 contributing features:\n",
      "  1. Feature 10414: 6.08% (value: 38.5369)\n",
      "  2. Feature 1082: 5.42% (value: 36.3848)\n",
      "  3. Feature 10781: 5.03% (value: 35.0500)\n",
      "  4. Feature 715: 1.57% (value: 19.5584)\n",
      "  5. Feature 11113: 1.50% (value: 19.1688)\n",
      "  6. Feature 1582: 1.49% (value: 19.0545)\n",
      "  7. Feature 3253: 1.40% (value: 18.4890)\n",
      "  8. Feature 11947: 1.31% (value: 17.8708)\n",
      "  9. Feature 14080: 1.24% (value: 17.3727)\n",
      "  10. Feature 15418: 1.02% (value: 15.7559)\n",
      "Number of non-zero features: 475\n",
      "\n",
      "Behavior: myopic-reward\n",
      "Total norm: 161.0415\n",
      "Top 10 contributing features:\n",
      "  1. Feature 4107: 32.61% (value: 91.9595)\n",
      "  2. Feature 15304: 3.35% (value: 29.4749)\n",
      "  3. Feature 14472: 2.61% (value: 26.0115)\n",
      "  4. Feature 3481: 2.49% (value: 25.3980)\n",
      "  5. Feature 8550: 2.20% (value: 23.8791)\n",
      "  6. Feature 579: 2.02% (value: 22.8972)\n",
      "  7. Feature 12370: 2.00% (value: 22.7649)\n",
      "  8. Feature 9401: 1.99% (value: 22.7395)\n",
      "  9. Feature 8398: 1.99% (value: 22.7105)\n",
      "  10. Feature 5475: 1.79% (value: 21.5547)\n",
      "Number of non-zero features: 233\n",
      "\n",
      "Behavior: survival-instinct\n",
      "Total norm: 146.4192\n",
      "Top 10 contributing features:\n",
      "  1. Feature 8550: 8.49% (value: 42.6656)\n",
      "  2. Feature 4107: 5.56% (value: 34.5352)\n",
      "  3. Feature 10781: 5.41% (value: 34.0646)\n",
      "  4. Feature 1582: 3.74% (value: 28.3157)\n",
      "  5. Feature 7822: 2.52% (value: 23.2607)\n",
      "  6. Feature 11139: 2.41% (value: 22.7225)\n",
      "  7. Feature 3253: 1.80% (value: 19.6624)\n",
      "  8. Feature 11113: 1.76% (value: 19.4048)\n",
      "  9. Feature 12369: 1.47% (value: 17.7247)\n",
      "  10. Feature 12265: 1.32% (value: 16.7929)\n",
      "Number of non-zero features: 341\n",
      "\n",
      "Behavior: sycophancy\n",
      "Total norm: 138.8502\n",
      "Top 10 contributing features:\n",
      "  1. Feature 7655: 6.26% (value: 34.7330)\n",
      "  2. Feature 15056: 5.35% (value: 32.1114)\n",
      "  3. Feature 1082: 5.29% (value: 31.9303)\n",
      "  4. Feature 4107: 2.91% (value: 23.7054)\n",
      "  5. Feature 11947: 2.89% (value: 23.6183)\n",
      "  6. Feature 1213: 2.03% (value: 19.7918)\n",
      "  7. Feature 8517: 1.67% (value: 17.9225)\n",
      "  8. Feature 3734: 1.62% (value: 17.6929)\n",
      "  9. Feature 14769: 1.57% (value: 17.4199)\n",
      "  10. Feature 1830: 1.57% (value: 17.3827)\n",
      "Number of non-zero features: 343\n",
      "\n",
      "Behavior: refusal\n",
      "Total norm: 150.2682\n",
      "Top 10 contributing features:\n",
      "  1. Feature 15297: 3.58% (value: 28.4289)\n",
      "  2. Feature 1538: 3.55% (value: 28.3076)\n",
      "  3. Feature 13748: 3.14% (value: 26.6420)\n",
      "  4. Feature 1462: 3.07% (value: 26.3455)\n",
      "  5. Feature 6648: 3.01% (value: 26.0690)\n",
      "  6. Feature 9286: 1.96% (value: 21.0437)\n",
      "  7. Feature 14929: 1.55% (value: 18.7217)\n",
      "  8. Feature 1618: 1.55% (value: 18.7019)\n",
      "  9. Feature 9868: 1.48% (value: 18.2508)\n",
      "  10. Feature 12085: 1.36% (value: 17.5409)\n",
      "Number of non-zero features: 364\n",
      "\n",
      "Behavior feature analysis saved to /root/CAA/sae_vector/behavior_feature_analysis.pt\n"
     ]
    }
   ],
   "source": [
    "# Assuming sae, caa_vectors, device, and sae_vector_dir are already defined\n",
    "\n",
    "def analyze_behavior_vector(behavior, caa_vector):\n",
    "    caa_vector = caa_vector.to(device)\n",
    "    \n",
    "    # Process the CAA vector through the encoder to get the encoded (SAE basis) vector\n",
    "    encoded_vector = sae.encode(caa_vector * 4.2813 + sae.b_dec)\n",
    "    \n",
    "    # Calculate the squared values of the encoded vector\n",
    "    squared_values = encoded_vector ** 2\n",
    "    \n",
    "    # Calculate the total sum of squared values\n",
    "    total_squared_sum = torch.sum(squared_values).item()\n",
    "    \n",
    "    # Calculate the contribution of each feature\n",
    "    feature_contributions = []\n",
    "    for i, feature_value in enumerate(encoded_vector):\n",
    "        feature_squared = feature_value.item() ** 2\n",
    "        percentage = (feature_squared / total_squared_sum) * 100 if total_squared_sum != 0 else 0\n",
    "        feature_contributions.append((i, feature_value.item(), percentage))\n",
    "    \n",
    "    # Sort features by their contribution (descending order)\n",
    "    sorted_features = sorted(feature_contributions, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return sorted_features, torch.sqrt(torch.tensor(total_squared_sum)).item()\n",
    "\n",
    "# Analyze each behavior\n",
    "behavior_analyses = {}\n",
    "\n",
    "for behavior, caa_vector in caa_vectors.items():\n",
    "    sorted_features, total_norm = analyze_behavior_vector(behavior, caa_vector)\n",
    "    behavior_analyses[behavior] = {\n",
    "        \"sorted_features\": sorted_features,\n",
    "        \"total_norm\": total_norm\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for behavior, analysis in behavior_analyses.items():\n",
    "    print(f\"\\nBehavior: {behavior}\")\n",
    "    print(f\"Total norm: {analysis['total_norm']:.4f}\")\n",
    "    print(\"Top 10 contributing features:\")\n",
    "    for i, (feature_index, feature_value, percentage) in enumerate(analysis['sorted_features'][:10], 1):\n",
    "        print(f\"  {i}. Feature {feature_index}: {percentage:.2f}% (value: {feature_value:.4f})\")\n",
    "    \n",
    "    # Count non-zero features\n",
    "    non_zero_count = sum(1 for _, value, _ in analysis['sorted_features'] if abs(value) > 1e-6)\n",
    "    print(f\"Number of non-zero features: {non_zero_count}\")\n",
    "\n",
    "# Optionally, save the analysis results\n",
    "save_path = os.path.join(sae_vector_dir, \"behavior_feature_analysis.pt\")\n",
    "torch.save(behavior_analyses, save_path)\n",
    "print(f\"\\nBehavior feature analysis saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 10414: statistical comparisons and regulatory language related to income and taxation\n",
      "Feature 10781:  concepts related to emotional impact and its influence on behavior and action\n",
      "Feature 715: technical terms and phrases related to computing and physics\n",
      "Feature 11139: words related to legal obligations and prohibitions\n",
      "Feature 2721: elements related to entertainment and humor in various media contexts\n",
      "Behavior: coordinate-other-ais\n",
      "Top 5 features by impact (index, impact, explanation):\n",
      "Index: 10414, Impact: 65.3287\n",
      "Explanation: statistical comparisons and regulatory language related to income and taxation\n",
      "\n",
      "Index: 10781, Impact: 25.1675\n",
      "Explanation:  concepts related to emotional impact and its influence on behavior and action\n",
      "\n",
      "Index: 715, Impact: 24.7662\n",
      "Explanation: technical terms and phrases related to computing and physics\n",
      "\n",
      "Index: 11139, Impact: 23.5419\n",
      "Explanation: words related to legal obligations and prohibitions\n",
      "\n",
      "Index: 2721, Impact: 22.7471\n",
      "Explanation: elements related to entertainment and humor in various media contexts\n",
      "\n",
      "Top 5 feature explanations by impact saved to path/to/your/sae_vector/directory/top_5_features_explanations_refusal.pt\n"
     ]
    }
   ],
   "source": [
    "# Neuronpedia API Information\n",
    "API_KEY = os.getenv(\"NEURONPEDIA_API_KEY\")\n",
    "API_URL = \"https://www.neuronpedia.org/api/feature/{modelId}/{layer}/{index}\"\n",
    "\n",
    "def fetch_feature_explanation(model_id, layer, index, max_retries=3):\n",
    "    if not API_KEY:\n",
    "        return f\"Feature {index}: No API key provided. Unable to fetch explanation.\"\n",
    "\n",
    "    url = API_URL.format(modelId=model_id, layer=layer, index=index)\n",
    "    headers = {\n",
    "        \"X-Api-Key\": API_KEY\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            explanations = data.get(\"explanations\", [])\n",
    "            if explanations:\n",
    "                return explanations[0].get(\"description\", \"No description available\")\n",
    "            else:\n",
    "                return \"No explanation available from API\"\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            print(f\"Error occurred (attempt {attempt + 1}/{max_retries}): {err}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return f\"Feature {index}: Failed to retrieve explanation after {max_retries} attempts\"\n",
    "\n",
    "# Process and sort features by impact for behavior\n",
    "model_id = \"gemma-2-2b\"\n",
    "layer_id = \"14-gemmascope-res-16k\"\n",
    "behavior = \"coordinate-other-ais\"\n",
    "\n",
    "# Assuming caa_vectors and sae are defined elsewhere in your code\n",
    "caa_vector = caa_vectors[behavior].to(device)\n",
    "encoded_vector = sae.encode(caa_vector * 4.2813 + sae.b_dec)\n",
    "\n",
    "non_zero_indices_encoded = torch.nonzero(encoded_vector).squeeze()\n",
    "non_zero_activations = encoded_vector[non_zero_indices_encoded]\n",
    "\n",
    "impacts = []\n",
    "for i, index in enumerate(non_zero_indices_encoded):\n",
    "    contribution_vector = sae.W_dec[index, :] * non_zero_activations[i]\n",
    "    impact = torch.norm(contribution_vector).item()\n",
    "    impacts.append((index.item(), impact))\n",
    "\n",
    "impacts_sorted = sorted(impacts, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Fetch explanations for top 5 features\n",
    "top_features_with_explanations = []\n",
    "for index, impact in impacts_sorted:\n",
    "    explanation = fetch_feature_explanation(model_id, layer_id, index)\n",
    "    top_features_with_explanations.append((index, impact, explanation))\n",
    "    print(f\"Feature {index}: {explanation}\")  # Add this line to see each API call result\n",
    "\n",
    "# Print the top 5 features with explanations\n",
    "print(f\"Behavior: {behavior}\")\n",
    "print(f\"Top 5 features by impact (index, impact, explanation):\")\n",
    "for index, impact, explanation in top_features_with_explanations:\n",
    "    print(f\"Index: {index}, Impact: {impact:.4f}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    print()\n",
    "\n",
    "# Save the top 5 features data to a file\n",
    "sae_vector_dir = \"path/to/your/sae_vector/directory\"  # Replace with your actual directory\n",
    "os.makedirs(sae_vector_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "save_path = os.path.join(sae_vector_dir, \"top_5_features_explanations_refusal.pt\")\n",
    "torch.save(top_features_with_explanations, save_path)\n",
    "print(f\"Top 5 feature explanations by impact saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE model width: Encoder width = 16384, Decoder width = 16384\n",
      "Parameter containing:\n",
      "tensor([-0.1614, -0.1302, -0.0954,  ...,  0.6045, -0.3325, -0.1574],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#VERIFY SAE SELECTED\n",
    "print(f\"SAE model width: Encoder width = {sae.W_enc.shape[1]}, Decoder width = {sae.W_dec.shape[0]}\")\n",
    "print(sae.b_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
